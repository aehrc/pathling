"use strict";(globalThis.webpackChunkpathling_site=globalThis.webpackChunkpathling_site||[]).push([[2120],{5948:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"libraries/examples/streaming","title":"Streaming queries","description":"Examples of running streaming queries over FHIR data using the Pathling libraries.","source":"@site/docs/libraries/examples/streaming.md","sourceDirName":"libraries/examples","slug":"/libraries/examples/streaming","permalink":"/docs/libraries/examples/streaming","draft":false,"unlisted":false,"editUrl":"https://github.com/aehrc/pathling/tree/main/site/docs/libraries/examples/streaming.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Streaming queries","description":"Examples of running streaming queries over FHIR data using the Pathling libraries."},"sidebar":"libraries","previous":{"title":"Grouping and analysing SNOMED CT data","permalink":"/docs/libraries/examples/grouping-snomed"},"next":{"title":"Adding display terms to codes","permalink":"/docs/libraries/examples/display-terms"}}');var t=i(4848),r=i(8453);const a={sidebar_position:3,title:"Streaming queries",description:"Examples of running streaming queries over FHIR data using the Pathling libraries."},o="Streaming queries",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Setting up the Spark environment",id:"setting-up-the-spark-environment",level:2},{value:"Consuming FHIR bundles from Kafka",id:"consuming-fhir-bundles-from-kafka",level:2},{value:"Defining SQL on FHIR views",id:"defining-sql-on-fhir-views",level:2},{value:"Patient demographics view",id:"patient-demographics-view",level:3},{value:"Diagnosis view with terminology operations",id:"diagnosis-view-with-terminology-operations",level:3},{value:"Encounter view with complex nested data",id:"encounter-view-with-complex-nested-data",level:3},{value:"Connecting to a terminology server",id:"connecting-to-a-terminology-server",level:2},{value:"Persisting to PostgreSQL",id:"persisting-to-postgresql",level:2},{value:"Orchestrating the complete pipeline",id:"orchestrating-the-complete-pipeline",level:2},{value:"Key benefits",id:"key-benefits",level:2},{value:"Deployment considerations",id:"deployment-considerations",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"streaming-queries",children:"Streaming queries"})}),"\n",(0,t.jsx)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/e-hFKckQITw?si=fzgfFzdB8C0X4RIX",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:!0}),"\n",(0,t.jsx)(n.p,{children:"Pathling supports streaming data sources, and all the operations available\nwithin the library are able to execute continuously across a stream of data."}),"\n",(0,t.jsxs)(n.p,{children:["The following demonstrates streaming FHIR data\nfrom ",(0,t.jsx)(n.a,{href:"https://kafka.apache.org/",children:"Kafka"}),", encoding the data, and performing\nterminology operations using Python:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from pathling import PathlingContext, Coding, subsumes\n\npc = PathlingContext.create()\n\n# Subscribe to a stream of FHIR Bundles from a Kafka topic.\ndf = (\n    pc.spark.readStream.format("kafka")\n    .option("kafka.bootstrap.servers", "kafka:9092")\n    .option("subscribe", "some-topic")\n    .load()\n    .selectExpr("CAST(value AS STRING) as json_bundle")\n)\n\n# Pull out the MedicationAdministration resources and put them into a dataset.\nmed_administrations = (\n    pc.encode_bundle(df, "MedicationAdministration")\n    .selectExpr(\n        "id", "status",\n        "EXPLODE_OUTER(medicationCodeableConcept.coding) as coding"\n    )\n)\n\n# Perform a subsumes operation on the medication coding to determine whether it is a type of\n# anti-coagulant.\nresult = med_administrations.select(\n    med_administrations.id,\n    med_administrations.status,\n    med_administrations.coding,\n    subsumes(\n        # 372862008 |Anticoagulant|\n        left_coding=Coding("http://snomed.info/sct", "372862008"),\n        right_coding_column="coding",\n    ).alias("is_anticoagulant"),\n)\n'})}),"\n",(0,t.jsxs)(n.p,{children:["For more information about Spark's Kafka integration, see\nthe ",(0,t.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html",children:"Structured Streaming + Kafka Integration Guide"}),"."]}),"\n",(0,t.jsx)(n.h1,{id:"worked-example-fhir-etl-pipeline-with-sql-on-fhir",children:"Worked example: FHIR ETL pipeline with SQL on FHIR"}),"\n",(0,t.jsx)(n.p,{children:"This implementation shows an ETL (Extract, Transform, Load) pipeline that\nconsumes FHIR resources from Kafka, transforms them using SQL on FHIR queries\nwith terminology operations, and stores the results to PostgreSQL. This\nexample illustrates how Pathling enables streaming analytics over clinical data."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline processes data in real time by:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Consumes FHIR bundles from Kafka topics"}),"\n",(0,t.jsx)(n.li,{children:"Extracting and encoding specific resource types (Patient, Encounter,\nCondition)"}),"\n",(0,t.jsxs)(n.li,{children:["Transforming the data using SQL on FHIR views with ",(0,t.jsx)(n.a,{href:"/docs/fhirpath",children:"FHIRPath"}),"\nexpressions"]}),"\n",(0,t.jsx)(n.li,{children:"Performing terminology operations including code translation and concept\nsubsumption"}),"\n",(0,t.jsx)(n.li,{children:"Storing the transformed data to PostgreSQL using an upsert strategy (\ninserting new records or updating existing ones)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"setting-up-the-spark-environment",children:"Setting up the Spark environment"}),"\n",(0,t.jsx)(n.p,{children:"The initial step involves configuring Spark with the required dependencies for\nFHIR\nprocessing, Kafka connectivity, and database integration:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from pathling import PathlingContext\nfrom pyspark.sql import SparkSession\nfrom pathling._version import __java_version__, __scala_version__\nfrom pyspark import __version__ as __spark_version__\n\n\ndef _get_or_create_spark() -> SparkSession:\n    """Create a Spark session configured for FHIR data processing."""\n    spark_builder = SparkSession.builder.config(\n        "spark.jars.packages",\n        f"org.apache.spark:spark-sql-kafka-0-10_{__scala_version__}:{__spark_version__},"\n        f"au.csiro.pathling:library-runtime:{__java_version__},"\n        f"org.postgresql:postgresql:42.2.18",\n    ).config("spark.sql.streaming.checkpointLocation", "/path/to/checkpoints")\n    return spark_builder.getOrCreate()\n'})}),"\n",(0,t.jsx)(n.p,{children:"This configuration incorporates:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Kafka connector"}),": For consuming streaming data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Pathling library"}),": For FHIR encoding and SQL on FHIR operations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PostgreSQL driver"}),": For database persistence"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Checkpoint location"}),": For fault-tolerant streaming (enabling recovery from\nfailures)"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"consuming-fhir-bundles-from-kafka",children:"Consuming FHIR bundles from Kafka"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline subscribes to a Kafka topic containing FHIR bundles and converts\nthese into typed resource streams:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def _subscribe_to_kafka_topic(spark: SparkSession,\n                              bootstrap_servers: str,\n                              topic: str) -> DataFrame:\n    """Subscribe to a Kafka topic as a streaming source."""\n    return (\n        spark.readStream.format("kafka")\n        .option("kafka.bootstrap.servers", bootstrap_servers)\n        .option("subscribe", topic)\n        .option("startingOffsets", "earliest")\n        .load()\n    )\n\n\ndef _to_resource_stream(kafka_stream: DataFrame,\n                        resource_type: str,\n                        pc: PathlingContext) -> DataFrame:\n    """Convert raw Kafka stream into typed FHIR resource stream."""\n    from pyspark.sql.functions import explode, from_json\n\n    json_stream = (\n        kafka_stream.selectExpr("CAST(value AS STRING) AS bundle")\n        .select(\n            explode(\n                from_json(\n                    "bundle",\n                    "STRUCT<entry:ARRAY<STRUCT<resource:STRING>>>",\n                ).entry.resource\n            ).alias("resource")\n        )\n        .filter(\n            from_json(\n                "resource",\n                "STRUCT<resourceType:STRING>",\n            ).resourceType == resource_type\n        )\n    )\n    return pc.encode(json_stream, resource_type)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This conversion process:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Reads raw messages from Kafka"}),"\n",(0,t.jsx)(n.li,{children:"Parses FHIR bundles from JSON"}),"\n",(0,t.jsx)(n.li,{children:"Extracts individual resources from bundle entries"}),"\n",(0,t.jsx)(n.li,{children:"Filters by resource type"}),"\n",(0,t.jsx)(n.li,{children:"Encodes resources using Pathling for FHIRPath query support"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"defining-sql-on-fhir-views",children:"Defining SQL on FHIR views"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline defines three views that transform raw FHIR resources into\nanalytical datasets. Each view uses FHIRPath expressions (a path-based query\nlanguage for FHIR) to extract and\ntransform clinical data."}),"\n",(0,t.jsx)(n.h3,{id:"patient-demographics-view",children:"Patient demographics view"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def view_patient(data: DataSource) -> DataFrame:\n    """Create a view of Patient resources with demographics."""\n    return data.view(\n        "Patient",\n        select=[{\n            "column": [\n                {\n                    "description": "Patient ID",\n                    "path": "getResourceKey()",\n                    "name": "id",\n                    "type": "string",\n                },\n                {\n                    "description": "Gender",\n                    "path": "gender",\n                    "name": "gender",\n                    "type": "code",\n                },\n            ],\n        }],\n    )\n'})}),"\n",(0,t.jsx)(n.h3,{id:"diagnosis-view-with-terminology-operations",children:"Diagnosis view with terminology operations"}),"\n",(0,t.jsx)(n.p,{children:"This view illustrates terminology processing capabilities:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def view_diagnosis(data: DataSource) -> DataFrame:\n    """Create a view of Condition resources with terminology operations."""\n    return data.view(\n        "Condition",\n        select=[{\n            "column": [\n                {\n                    "description": "Condition ID",\n                    "path": "getResourceKey()",\n                    "name": "id",\n                },\n                {\n                    "description": "Patient ID",\n                    "path": "subject.getReferenceKey()",\n                    "name": "patient_id",\n                },\n                {\n                    "description": "SNOMED CT diagnosis code",\n                    "path": "code.coding.where(system = \'http://snomed.info/sct\').code",\n                    "name": "sct_id",\n                },\n                {\n                    "description": "ICD 10-AM diagnosis code",\n                    "path": "code.translate(\'http://aehrc.com/fhir/ConceptMap/aehrc-snomap-starter\', false, \'wider\').first().code",\n                    "name": "icd10am_code",\n                },\n                {\n                    "description": "Viral infection",\n                    "path": "code.subsumedBy(http://snomed.info/sct|34014006 combine http://snomed.info/sct|438508001)",\n                    "name": "viral_infection",\n                    "type": "boolean",\n                },\n            ],\n        }],\n    )\n'})}),"\n",(0,t.jsx)(n.p,{children:"Key features:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code extraction"}),": Filters codings by system to extract SNOMED CT codes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code translation"}),": Uses ",(0,t.jsx)(n.a,{href:"/docs/fhirpath#translate",children:(0,t.jsx)(n.code,{children:"translate"})})," to map\nSNOMED CT to ICD-10-AM via a ConceptMap"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Concept subsumption"}),": Uses ",(0,t.jsx)(n.a,{href:"/docs/fhirpath#subsumedby",children:(0,t.jsx)(n.code,{children:"subsumedBy"})})," to\ndetect if a condition is a viral infection by checking against parent concepts"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"encounter-view-with-complex-nested-data",children:"Encounter view with complex nested data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def view_encounter(data: DataSource) -> DataFrame:\n    """Create a comprehensive view of Encounter resources."""\n    return data.view(\n        "Encounter",\n        select=[{\n            "column": [\n                {\n                    "path": "getResourceKey()",\n                    "name": "id",\n                },\n                {\n                    "path": "subject.getReferenceKey()",\n                    "name": "patient_id",\n                },\n                {\n                    "path": "period.start",\n                    "name": "start_time",\n                    "type": "dateTime",\n                },\n            ],\n            "select": [\n                {\n                    "forEachOrNull": "type.coding.where(system = \'http://occio.qh/data/typeofvisit\')",\n                    "column": [\n                        {\n                            "path": "code",\n                            "name": "type_of_visit_code",\n                        },\n                        {\n                            "path": "display",\n                            "name": "type_of_visit_desc",\n                        },\n                    ],\n                },\n                {\n                    "forEachOrNull": "priority.coding.where(system = \'http://occio.qh/data/ats\')",\n                    "column": [\n                        {\n                            "path": "code",\n                            "name": "ats_code",\n                        },\n                    ],\n                },\n                {\n                    "forEachOrNull": "diagnosis.where(use.coding.exists(system = \'Admission diagnosis\'))",\n                    "column": [\n                        {\n                            "path": "condition.getReferenceKey()",\n                            "name": "admission_diagnosis_id",\n                        },\n                    ],\n                },\n            ],\n        }],\n    )\n'})}),"\n",(0,t.jsx)(n.p,{children:"This view demonstrates:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nested selections"}),": Using ",(0,t.jsx)(n.code,{children:"forEachOrNull"})," to handle optional complex types"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Filtered extractions"}),": Extracting specific codings based on system URLs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conditional logic"}),": Finding admission diagnoses using FHIRPath predicates"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"connecting-to-a-terminology-server",children:"Connecting to a terminology server"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline connects to a terminology server for real-time code validation and\ntranslation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'pc = PathlingContext.create(\n    spark,\n    terminology_server_url="https://terminology-service/fhir",\n)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This enables:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Live code validation during streaming"}),"\n",(0,t.jsx)(n.li,{children:"Dynamic concept map translations"}),"\n",(0,t.jsx)(n.li,{children:"Subsumption testing against terminology hierarchies"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"persisting-to-postgresql",children:"Persisting to PostgreSQL"}),"\n",(0,t.jsx)(n.p,{children:"The pipeline implements an upsert pattern to maintain up-to-date views in\nPostgreSQL:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def write_postgresql(df: DataFrame, db_name: str, schema: str, view_name: str):\n    """Persist DataFrame to PostgreSQL using upsert strategy."""\n    import psycopg2\n\n    columns = df.columns\n    insert_columns = ", ".join(columns)\n    insert_values = ", ".join(["%s"] * len(columns))\n    # Exclude \'id\' from update to avoid changing primary key\n    update_set = ", ".join(\n        [f"{col} = EXCLUDED.{col}" for col in columns if col != "id"]\n    )\n\n    sql = f"""\n    INSERT INTO {schema}.{view_name} ({insert_columns})\n    VALUES ({insert_values})\n    ON CONFLICT (id) DO UPDATE SET {update_set}\n    """\n\n    def upsert_partition(partition):\n        conn = psycopg2.connect(\n            host=host, database=db_name, user=user, password=password\n        )\n        cursor = conn.cursor()\n        data = list(partition)\n        if data:\n            cursor.executemany(sql, data)\n            conn.commit()\n        cursor.close()\n        conn.close()\n\n    df.foreachPartition(upsert_partition)\n'})}),"\n",(0,t.jsx)(n.p,{children:"This method:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Inserts new records"}),"\n",(0,t.jsx)(n.li,{children:"Updates existing records when IDs match"}),"\n",(0,t.jsx)(n.li,{children:"Maintains data consistency across streaming updates"}),"\n",(0,t.jsx)(n.li,{children:"Processes data in partitions for efficiency"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"orchestrating-the-complete-pipeline",children:"Orchestrating the complete pipeline"}),"\n",(0,t.jsx)(n.p,{children:"The main consumer function brings all components together:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def start_consumer(kafka_topic: str, kafka_bootstrap_servers: str,\n                   db_name: str, schema: str, host: str,\n                   user: str, password: str):\n    """Start the FHIR resource ETL pipeline from Kafka."""\n\n    spark = _get_or_create_spark()\n    pc = PathlingContext.create(\n        spark,\n        terminology_server_url="http://velonto-ontoserver-service/fhir",\n    )\n\n    # Subscribe to Kafka\n    update_stream = _subscribe_to_kafka_topic()\n\n    # Create resource streams for each type\n    data = pc.read.datasets({\n        resource_type: _to_resource_stream(update_stream, resource_type)\n        for resource_type in ["Patient", "Encounter", "Condition"]\n    })\n\n    # Define views\n    all_views = [view_patient, view_encounter, view_diagnosis]\n\n    # Create parallel sinks\n    console_sinks = []\n    postgresql_sinks = []\n\n    for view_f in all_views:\n        view_name = view_f.__name__\n        view_data = view_f(data)\n\n        # Console sink for monitoring\n        console_sink = (\n            view_data.writeStream.outputMode("append")\n            .format("console")\n            .start(f"console_{view_name}")\n        )\n        console_sinks.append(console_sink)\n\n        # PostgreSQL sink for persistence\n        postgresql_sink = (\n            view_data.writeStream.foreachBatch(\n                lambda df, _, view_name=view_name: write_postgresql(\n                    df, db_name, schema, view_name\n                )\n            )\n            .outputMode("append")\n            .start()\n        )\n        postgresql_sinks.append(postgresql_sink)\n\n    # Block until termination\n    for sink in console_sinks + postgresql_sinks:\n        sink.awaitTermination()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"key-benefits",children:"Key benefits"}),"\n",(0,t.jsx)(n.p,{children:"This architecture enables:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time processing"}),": Continuous processing of FHIR data as it arrives"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex transformations"}),": SQL on FHIR queries with FHIRPath expressions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Terminology integration"}),": Live code validation and translation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fault tolerance"}),": Spark checkpointing enables recovery from failures"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalability"}),": Distributed processing across Spark clusters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data persistence"}),": Maintains current analytical views in PostgreSQL"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"deployment-considerations",children:"Deployment considerations"}),"\n",(0,t.jsx)(n.p,{children:"When deploying this pipeline in production:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Checkpointing"}),": Configure checkpoint directories on reliable storage (HDFS,\nS3)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error handling"}),": Implement dead letter queues for malformed messages"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitoring"}),": Set up metrics for lag, throughput, and error rates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scaling"}),": Adjust Spark executor counts based on data volume"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Schema evolution"}),": Plan for FHIR profile changes and version updates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Security"}),": Secure Kafka connections with SSL/SASL and database credentials"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This example shows how Pathling enables real-time analytics\nover FHIR data streams by combining SQL on FHIR query capabilities with\nApache Spark and Kafka scalability."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);