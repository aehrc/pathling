"use strict";(globalThis.webpackChunkpathling_site=globalThis.webpackChunkpathling_site||[]).push([[9866],{1470:(e,n,t)=>{t.d(n,{A:()=>y});var a=t(6540),r=t(4164),s=t(7559),o=t(3104),i=t(6347),l=t(205),c=t(7485),d=t(1682),u=t(679);function p(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return p(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function b({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const t=(0,i.W6)(),r=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(r),(0,a.useCallback)(e=>{if(!r)return;const n=new URLSearchParams(t.location.search);n.set(r,e),t.replace({...t.location,search:n.toString()})},[r,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:r}=e,s=h(e),[o,i]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!b({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[c,d]=g({queryString:t,groupId:r}),[p,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,r]=(0,u.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&r.set(e)},[n,r])]}({groupId:r}),m=(()=>{const e=c??p;return b({value:e,tabValues:s})?e:null})();(0,l.A)(()=>{m&&i(m)},[m]);return{selectedValue:o,selectValue:(0,a.useCallback)(e=>{if(!b({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);i(e),d(e),f(e)},[d,f,s]),tabValues:s}}var m=t(2303);const v={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(4848);function j({className:e,block:n,selectedValue:t,selectValue:a,tabValues:s}){const i=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,r=i.indexOf(n),o=s[r].value;o!==t&&(l(n),a(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,r.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:a})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:c,...a,className:(0,r.A)("tabs__item",v.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function k({lazy:e,children:n,selectedValue:t}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,r.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function S(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,r.A)(s.G.tabs.container,"tabs-container",v.tabList),children:[(0,x.jsx)(j,{...n,...e}),(0,x.jsx)(k,{...n,...e})]})}function y(e){const n=(0,m.A)();return(0,x.jsx)(S,{...e,children:p(e.children)},String(n))}},2190:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>h,frontMatter:()=>l,metadata:()=>a,toc:()=>u});const a=JSON.parse('{"id":"libraries/io/encoders","title":"FHIR encoders","description":"The Pathling library can be used to transform FHIR Bundles or NDJSON into Spark data sets.","source":"@site/docs/libraries/io/encoders.md","sourceDirName":"libraries/io","slug":"/libraries/io/encoders","permalink":"/docs/libraries/io/encoders","draft":false,"unlisted":false,"editUrl":"https://github.com/aehrc/pathling/tree/main/site/docs/libraries/io/encoders.md","tags":[],"version":"current","frontMatter":{"description":"The Pathling library can be used to transform FHIR Bundles or NDJSON into Spark data sets."},"sidebar":"libraries","previous":{"title":"Data in and out","permalink":"/docs/libraries/io/"},"next":{"title":"Parquet specification","permalink":"/docs/libraries/io/schema"}}');var r=t(4848),s=t(8453),o=t(1470),i=t(9365);const l={description:"The Pathling library can be used to transform FHIR Bundles or NDJSON into Spark data sets."},c="FHIR encoders",d={},u=[{value:"Reading in NDJSON",id:"reading-in-ndjson",level:2},{value:"Reading in Bundles",id:"reading-in-bundles",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"fhir-encoders",children:"FHIR encoders"})}),"\n",(0,r.jsxs)(n.p,{children:["The Pathling library also includes a lower-level interface called the encoders.\nThe encoders can be used to transform ",(0,r.jsx)(n.a,{href:"https://hl7.org/fhir",children:"FHIR"})," Bundles or NDJSON into Spark\ndata sets. Once your data is encoded, it can be queried using SQL, or\ntransformed using the full library of functions that Spark provides. It can also\nbe written to ",(0,r.jsx)(n.a,{href:"https://parquet.apache.org/",children:"Parquet"})," and other formats that are\ncompatible with a wide range of tools. See\nthe ",(0,r.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/",children:"Spark documentation"})," for more\ndetails."]}),"\n","\n",(0,r.jsx)(n.h2,{id:"reading-in-ndjson",children:"Reading in NDJSON"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://hl7.org/fhir/R4/nd-json.html",children:"NDJSON"})," is a format commonly used for\nbulk FHIR data, and consists of files (one per resource type) that contains one\nJSON resource per line."]}),"\n",(0,r.jsxs)(o.A,{children:[(0,r.jsx)(i.A,{value:"python",label:"Python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from pathling import PathlingContext\n\npc = PathlingContext.create()\n\n# Read each line from the NDJSON into a row within a Spark data set.\nndjson_dir = '/some/path/ndjson/'\njson_resources = pc.spark.read.text(ndjson_dir)\n\n# Convert the data set of strings into a structured FHIR data set.\npatients = pc.encode(json_resources, 'Patient')\n\n# Do some stuff.\npatients.select('id', 'gender', 'birthDate').show()\n"})})}),(0,r.jsx)(i.A,{value:"r",label:"R",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-r",children:"library(sparklyr)\nlibrary(pathling)\n\npc <- pathling_connect()\n\nndjson <- '/some/path/ndjson/Condition.ndjson'\njson_resources <- pathling_spark(pc) %>% spark_read_text(ndjson)\n\npc %>% pathling_encode(json_resources, 'Condition') %>% show()\n\npc %>% pathling_disconnect()\n"})})}),(0,r.jsx)(i.A,{value:"scala",label:"Scala",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'import au.csiro.pathling.library.PathlingContext\n\nval spark = SparkSession.builder.getOrCreate()\n\n// Read each line from the NDJSON into a row within a Spark data set.\nval ndjsonDir = "/some/path/ndjson/"\nval jsonResources = spark.read.text(ndjsonDir)\n\n// Convert the data set of strings into a structured FHIR data set.\nval pc = PathlingContext.create(spark)\nval patients = pc.encode(jsonResources, "Patient")\n\n// Do some stuff.\npatients.select("id", "gender", "birthDate").show()\n'})})}),(0,r.jsx)(i.A,{value:"java",label:"Java",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'import org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.Dataset;\nimport au.csiro.pathling.library.PathlingContext;\n\nclass MyApp {\n\n    public static void main(String args[]) {\n        SparkSession spark = SparkSession.builder().getOrCreate();\n\n        // Read each line from the NDJSON into a row within a Spark data set.\n        String ndjsonDir = "/some/path/ndjson/";\n        Dataset<Row> jsonResources = spark.read().text(ndjsonDir);\n\n        // Convert the data set of strings into a structured FHIR data set.\n        PathlingContext pc = PathlingContext.create(spark);\n        Dataset<Row> patients = pc.encode(jsonResources, "Patient");\n\n        // Do some stuff.\n        patients.select("id", "gender", "birthDate").show();\n    }\n\n}\n'})})})]}),"\n",(0,r.jsx)(n.h2,{id:"reading-in-bundles",children:"Reading in Bundles"}),"\n",(0,r.jsxs)(n.p,{children:["The FHIR ",(0,r.jsx)(n.a,{href:"https://hl7.org/fhir/R4/bundle.html",children:"Bundle"})," resource can contain a\ncollection of FHIR resources. It is often used to represent a set of related\nresources, perhaps generated as part of the same event."]}),"\n",(0,r.jsxs)(o.A,{children:[(0,r.jsx)(i.A,{value:"python",label:"Python",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from pathling import PathlingContext\n\npc = PathlingContext.create()\n\n# Read each Bundle into a row within a Spark data set.\nbundles_dir = '/some/path/bundles/'\nbundles = pc.spark.read.text(bundles_dir, wholetext=True)\n\n# Convert the data set of strings into a structured FHIR data set.\npatients = pc.encode_bundle(bundles, 'Patient')\n\n# JSON is the default format, XML Bundles can be encoded using input type.\n# patients = pc.encodeBundle(bundles, 'Patient', inputType=MimeType.FHIR_XML)\n\n# Do some stuff.\npatients.select('id', 'gender', 'birthDate').show()\n"})})}),(0,r.jsx)(i.A,{value:"r",label:"R",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-r",children:"library(sparklyr)\nlibrary(pathling)\n\npc <- pathling_connect()\n\nbundles_dir <- '/some/path/bundles'\njson_bundles <- pathling_spark(pc) %>% spark_read_text(bundles_dir, whole = TRUE)\n\npc %>% pathling_encode_bundle(json_bundles, 'Condition', column = 'contents') %>% show()\n\npc %>% pathling_disconnect()\n"})})}),(0,r.jsx)(i.A,{value:"scala",label:"Scala",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'import org.apache.spark.sql.SparkSession\nimport au.csiro.pathling.library.PathlingContext\n\nval spark = SparkSession.builder.getOrCreate()\n\n// Read each line from the NDJSON into a row within a Spark data set.\nval bundlesDir = "/some/path/bundles/"\nval bundles = spark.read.option("wholetext", value = true).text(bundlesDir)\n\n// Convert the data set of strings into a structured FHIR data set.\nval pc = PathlingContext.create(spark)\nval patients = pc.encodeBundle(bundles, "Patient")\n\n// JSON is the default format, XML Bundles can be encoded using input type.\n// val patients = pc.encodeBundle(bundles, "Patient", FhirMimeTypes.FHIR_XML)\n\n// Do some stuff.\npatients.select("id", "gender", "birthDate").show()\n'})})}),(0,r.jsx)(i.A,{value:"java",label:"Java",children:(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'import org.apache.spark.sql.SparkSession;\nimport org.apache.spark.sql.Dataset;\nimport au.csiro.pathling.library.PathlingContext;\n\nclass MyApp {\n\n    public static void main(String args[]) {\n        SparkSession spark = SparkSession.builder().getOrCreate();\n\n        // Read each line from the NDJSON into a row within a Spark data set.\n        String bundlesDir = "/some/path/bundles/";\n        Dataset<Row> bundles = spark.read()\n                .option("wholetext", true)\n                .text(bundlesDir);\n\n        // Convert the data set of strings into a structured FHIR data set.\n        PathlingContext pc = PathlingContext.create(spark);\n        Dataset<Row> patients = pc.encodeBundle(bundles, "Patient");\n\n        // JSON is the default format, XML Bundles can be encoded using input \n        // type.\n        // Dataset<Row> patients = pc.encodeBundle(bundles, "Patient", \n        //     FhirMimeTypes.FHIR_XML);\n\n        // Do some stuff.\n        patients.select("id", "gender", "birthDate").show();\n    }\n\n}\n'})})})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>i});var a=t(6540);const r={},s=a.createContext(r);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},9365:(e,n,t)=>{t.d(n,{A:()=>o});t(6540);var a=t(4164);const r={tabItem:"tabItem_Ymn6"};var s=t(4848);function o({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(r.tabItem,t),hidden:n,children:e})}}}]);