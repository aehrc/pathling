"use strict";(self.webpackChunkpathling_site=self.webpackChunkpathling_site||[]).push([[2623],{4967:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"libraries/examples/fhir-server-sync","title":"FHIR server synchronisation","description":"Examples of synchronising your analytic data store with a FHIR server.","source":"@site/docs/libraries/examples/fhir-server-sync.md","sourceDirName":"libraries/examples","slug":"/libraries/examples/fhir-server-sync","permalink":"/docs/libraries/examples/fhir-server-sync","draft":false,"unlisted":false,"editUrl":"https://github.com/aehrc/pathling/tree/main/site/docs/libraries/examples/fhir-server-sync.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"FHIR server synchronisation","description":"Examples of synchronising your analytic data store with a FHIR server."},"sidebar":"libraries","previous":{"title":"Prostate cancer risk factors","permalink":"/docs/libraries/examples/prostate-cancer"},"next":{"title":"Grouping and analysing SNOMED CT data","permalink":"/docs/libraries/examples/grouping-snomed"}}');var r=t(4848),a=t(8453);const s={sidebar_position:2,title:"FHIR server synchronisation",description:"Examples of synchronising your analytic data store with a FHIR server."},o="FHIR server synchronisation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Authentication",id:"authentication",level:3},{value:"Synchronisation script",id:"synchronisation-script",level:2},{value:"Scheduling periodic synchronisation",id:"scheduling-periodic-synchronisation",level:2},{value:"Using cron",id:"using-cron",level:3},{value:"Using Python scheduler",id:"using-python-scheduler",level:3},{value:"Using Apache Airflow",id:"using-apache-airflow",level:3},{value:"Monitoring and health checks",id:"monitoring-and-health-checks",level:2},{value:"Production considerations",id:"production-considerations",level:2},{value:"Security considerations",id:"security-considerations",level:3},{value:"Large dataset configuration",id:"large-dataset-configuration",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"fhir-server-synchronisation",children:"FHIR server synchronisation"})}),"\n",(0,r.jsx)(n.p,{children:"This guide describes how to create a synchronisation pipeline that periodically\nretrieves data from a FHIR bulk export endpoint and maintains a set of Delta\ntables for analysis."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"The FHIR (Fast Healthcare Interoperability Resources) Bulk Data Access\nspecification defines a standard method for extracting large volumes of\nhealthcare data from FHIR servers. When combined with Pathling's Delta Lake\nsupport, this approach enables data pipelines with the following\ncharacteristics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Initial full data exports establish baseline datasets"}),"\n",(0,r.jsxs)(n.li,{children:["Incremental exports use the ",(0,r.jsx)(n.code,{children:"since"})," parameter to retrieve only changed records"]}),"\n",(0,r.jsx)(n.li,{children:"Data merging occurs automatically through Pathling's merge functionality"}),"\n",(0,r.jsx)(n.li,{children:"Error handling and retry mechanisms are built into the bulk client"}),"\n",(0,r.jsx)(n.li,{children:"Delta Lake provides transactional consistency for data operations"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"authentication",children:"Authentication"}),"\n",(0,r.jsxs)(n.p,{children:["The synchronisation process\nuses ",(0,r.jsx)(n.a,{href:"https://www.hl7.org/fhir/smart-app-launch/backend-services.html",children:"SMART Backend Services"}),"\nwith the ",(0,r.jsx)(n.code,{children:"client-confidential-asymmetric"})," profile for authentication. This\nauthentication method was designed for automated backend services that require\nunattended access to FHIR resources. Common applications include:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Analytics platforms importing bulk healthcare data"}),"\n",(0,r.jsx)(n.li,{children:"Data integration services synchronising patient records"}),"\n",(0,r.jsx)(n.li,{children:"Public health surveillance systems"}),"\n",(0,r.jsx)(n.li,{children:"Healthcare utilisation tracking systems"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This authentication approach uses asymmetric cryptography (public and private\nkey pairs) instead of shared secrets, which reduces security risks in automated\nenvironments that handle sensitive healthcare information."}),"\n",(0,r.jsx)(n.h2,{id:"synchronisation-script",children:"Synchronisation script"}),"\n",(0,r.jsx)(n.p,{children:"The following Python script demonstrates the synchronisation process using\nPathling's bulk export and Delta merge functionality:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nFHIR Bulk Export to Delta Lake Synchronisation Script\n\nThis script performs periodic synchronisation of FHIR data from a bulk export\nendpoint to Delta tables, supporting both initial loads and incremental updates.\n"""\n\nimport json\nimport logging\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Any\n\nfrom pathling import PathlingContext\nfrom pathling.datasink import SaveMode\n\n# Configure logging.\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\n)\nlogger = logging.getLogger(__name__)\n\n\nclass SyncState:\n    """Tracks the state of synchronisation for incremental updates."""\n\n    def __init__(self, state_file: Path):\n        self.state_file = state_file\n        self.last_sync_time: Optional[datetime] = None\n        self.load()\n\n    def load(self):\n        """Load sync state from JSON file."""\n        if self.state_file.exists():\n            with open(self.state_file, \'r\') as f:\n                data = json.load(f)\n                if data.get(\'last_sync_time\'):\n                    self.last_sync_time = datetime.fromisoformat(\n                        data[\'last_sync_time\']\n                    )\n\n    def save(self):\n        """Save sync state to JSON file."""\n        self.state_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(self.state_file, \'w\') as f:\n            json.dump({\n                \'last_sync_time\': self.last_sync_time.isoformat()\n                if self.last_sync_time else None\n            }, f, indent=2)\n\n\ndef sync_fhir_to_delta(\n        fhir_endpoint: str,\n        delta_path: str,\n        state_file: Path,\n        resource_types: Optional[List[str]] = None,\n        group_id: Optional[str] = None,\n        auth_config: Optional[Dict[str, Any]] = None,\n        full_refresh: bool = False\n):\n    """\n    Synchronise FHIR data from bulk export endpoint to Delta tables.\n    \n    Args:\n        fhir_endpoint: FHIR server bulk export endpoint URL\n        delta_path: Base path for Delta tables\n        state_file: Path to store sync state\n        resource_types: List of resource types to sync\n        group_id: Optional group ID for group-level export\n        auth_config: Authentication configuration for FHIR server\n        full_refresh: Force a full export instead of incremental\n    """\n    start_time = datetime.now(timezone.utc)\n\n    # Load sync state.\n    state = SyncState(state_file)\n\n    # Initialise Pathling context with Delta support.\n    pc = PathlingContext.create(enable_delta=True)\n\n    logger.info(f"Starting sync from {fhir_endpoint}")\n    if state.last_sync_time and not full_refresh:\n        logger.info(f"Incremental sync since {state.last_sync_time}")\n\n    # Determine export parameters.\n    since = None if full_refresh else state.last_sync_time\n\n    # Execute bulk export to retrieve data from FHIR server.\n    # Retry logic is handled internally by the bulk client.\n    data_source = pc.read.bulk(\n        fhir_endpoint_url=fhir_endpoint,\n        group_id=group_id,\n        types=resource_types,\n        since=since,\n        auth_config=auth_config,\n        timeout=3600  # 1 hour timeout.\n    )\n\n    # Save data to Delta tables using merge operation.\n    # The merge process handles both initial table creation and updates to existing data.\n    data_source.write.delta(delta_path, save_mode=SaveMode.MERGE)\n\n    # Update sync state.\n    state.last_sync_time = start_time\n    state.save()\n\n    duration = (datetime.now(timezone.utc) - start_time).total_seconds()\n    logger.info(f"Sync completed successfully in {duration:.2f} seconds")\n\n\nif __name__ == "__main__":\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description="Sync FHIR data from bulk export to Delta tables"\n    )\n    parser.add_argument(\n        "--fhir-endpoint",\n        required=True,\n        help="FHIR server bulk export endpoint URL"\n    )\n    parser.add_argument(\n        "--delta-path",\n        required=True,\n        help="Base path for Delta tables"\n    )\n    parser.add_argument(\n        "--state-file",\n        default="/var/lib/fhir-sync/state.json",\n        help="Path to sync state file"\n    )\n    parser.add_argument(\n        "--resource-types",\n        nargs="+",\n        default=["Patient", "Condition", "Observation", "Encounter"],\n        help="Resource types to sync"\n    )\n    parser.add_argument(\n        "--group-id",\n        help="Group ID for group-level export"\n    )\n    parser.add_argument(\n        "--full-refresh",\n        action="store_true",\n        help="Force a full export instead of incremental"\n    )\n    parser.add_argument(\n        "--auth-client-id",\n        help="OAuth2 client ID for authentication"\n    )\n    parser.add_argument(\n        "--auth-private-key",\n        help="Path to private key file for authentication"\n    )\n\n    args = parser.parse_args()\n\n    # Build authentication configuration if provided.\n    auth_config = None\n    if args.auth_client_id:\n        auth_config = {\n            "enabled": True,\n            "client_id": args.auth_client_id,\n            "use_smart": True,\n            "scope": "system/*.read"\n        }\n\n        if args.auth_private_key:\n            with open(args.auth_private_key, \'r\') as f:\n                auth_config["private_key_jwk"] = f.read()\n\n    # Run synchronisation.\n    sync_fhir_to_delta(\n        fhir_endpoint=args.fhir_endpoint,\n        delta_path=args.delta_path,\n        state_file=Path(args.state_file),\n        resource_types=args.resource_types,\n        group_id=args.group_id,\n        auth_config=auth_config,\n        full_refresh=args.full_refresh\n    )\n'})}),"\n",(0,r.jsx)(n.h2,{id:"scheduling-periodic-synchronisation",children:"Scheduling periodic synchronisation"}),"\n",(0,r.jsx)(n.h3,{id:"using-cron",children:"Using cron"}),"\n",(0,r.jsx)(n.p,{children:"Periodic synchronisation can be scheduled using cron. The following shell script\nprovides a basic wrapper:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# /usr/local/bin/fhir-sync.sh\n\n# Set up environment.\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk\nexport SPARK_HOME=/opt/spark\n\n# Run the sync script.\n/usr/bin/python3 /opt/fhir-sync/sync.py \\\n    --fhir-endpoint "https://your-fhir-server.com/fhir" \\\n    --delta-path "/data/delta/fhir" \\\n    --state-file "/var/lib/fhir-sync/state.json" \\\n    --resource-types Patient Condition Observation Encounter \\\n    --auth-client-id "your-client-id" \\\n    --auth-private-key "/etc/fhir-sync/private-key.jwk" \\\n    >> /var/log/fhir-sync/sync.log 2>&1\n'})}),"\n",(0,r.jsx)(n.p,{children:"Then add to crontab to run every hour:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Run FHIR sync every hour at 15 minutes past.\n15 * * * * /usr/local/bin/fhir-sync.sh\n"})}),"\n",(0,r.jsx)(n.h3,{id:"using-python-scheduler",children:"Using Python scheduler"}),"\n",(0,r.jsxs)(n.p,{children:["More complex scheduling requirements can be addressed using Python's ",(0,r.jsx)(n.code,{children:"schedule"}),"\nlibrary:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import schedule\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\n\n\ndef run_sync(full_refresh=False):\n    """Execute the sync process."""\n    logger.info(f"Starting scheduled sync at {datetime.now()}")\n\n    sync_fhir_to_delta(\n        fhir_endpoint="https://your-fhir-server.com/fhir",\n        delta_path="/data/delta/fhir",\n        state_file=Path("/var/lib/fhir-sync/state.json"),\n        resource_types=["Patient", "Condition", "Observation"],\n        full_refresh=full_refresh\n    )\n\n\n# Schedule different sync patterns.\nschedule.every().hour.at(":15").do(run_sync)  # Hourly incremental.\nschedule.every().sunday.at("03:00").do(\n    lambda: run_sync(full_refresh=True)\n)  # Weekly full refresh.\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(60)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"using-apache-airflow",children:"Using Apache Airflow"}),"\n",(0,r.jsx)(n.p,{children:"Enterprise environments may implement scheduling through Apache Airflow using a\nDAG (Directed Acyclic Graph):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.email import EmailOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-team',\n    'depends_on_past': False,\n    'start_date': datetime(2025, 1, 1),\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5)\n}\n\ndag = DAG(\n    'fhir_bulk_sync',\n    default_args=default_args,\n    description='Sync FHIR data to Delta tables',\n    schedule_interval='0 * * * *',  # Hourly.\n    catchup=False\n)\n\n\ndef sync_task(**context):\n    \"\"\"Airflow task to run sync.\"\"\"\n    from pathlib import Path\n\n    sync_fhir_to_delta(\n        fhir_endpoint=\"{{ var.value.fhir_endpoint }}\",\n        delta_path=\"{{ var.value.delta_path }}\",\n        state_file=Path(\"{{ var.value.state_file }}\"),\n        resource_types=[\"Patient\", \"Condition\", \"Observation\"]\n    )\n\n\nsync = PythonOperator(\n    task_id='sync_fhir_data',\n    python_callable=sync_task,\n    dag=dag\n)\n\nnotify = EmailOperator(\n    task_id='send_notification',\n    to=['data-team@example.com'],\n    subject='FHIR Sync Completed',\n    html_content='Sync completed at {{ ds }}',\n    trigger_rule='all_done',\n    dag=dag\n)\n\nsync >> notify\n"})}),"\n",(0,r.jsx)(n.h2,{id:"monitoring-and-health-checks",children:"Monitoring and health checks"}),"\n",(0,r.jsx)(n.p,{children:"Health check endpoints can be implemented for system monitoring:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from flask import Flask, jsonify\n\napp = Flask(__name__)\n\n\n@app.route('/health')\ndef health_check():\n    \"\"\"Health check endpoint for monitoring systems.\"\"\"\n    try:\n        state = SyncState(Path(\"/var/lib/fhir-sync/state.json\"))\n\n        # Check if sync is running regularly.\n        if state.last_sync_time:\n            time_since_sync = datetime.now(timezone.utc) - state.last_sync_time\n            is_healthy = time_since_sync.total_seconds() < 7200  # 2 hours.\n        else:\n            is_healthy = False\n\n        return jsonify({\n            'status': 'healthy' if is_healthy else 'unhealthy',\n            'last_sync': state.last_sync_time.isoformat() if state.last_sync_time else None\n        }), 200 if is_healthy else 503\n\n    except Exception as e:\n        return jsonify({\n            'status': 'error',\n            'message': str(e)\n        }), 503\n"})}),"\n",(0,r.jsx)(n.h2,{id:"production-considerations",children:"Production considerations"}),"\n",(0,r.jsx)(n.h3,{id:"security-considerations",children:"Security considerations"}),"\n",(0,r.jsx)(n.p,{children:"Credentials should be stored securely using environment variables or dedicated\nsecret management systems:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import os\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.identity import DefaultAzureCredential\n\n\ndef get_auth_config():\n    """Retrieve authentication config from secure storage."""\n    # Option 1: Environment variables.\n    if os.getenv("FHIR_CLIENT_ID"):\n        return {\n            "enabled": True,\n            "client_id": os.getenv("FHIR_CLIENT_ID"),\n            "private_key_jwk": os.getenv("FHIR_PRIVATE_KEY"),\n            "use_smart": True,\n            "scope": "system/*.read"\n        }\n\n    # Option 2: Azure Key Vault.\n    credential = DefaultAzureCredential()\n    client = SecretClient(\n        vault_url="https://your-vault.vault.azure.net/",\n        credential=credential\n    )\n\n    return {\n        "enabled": True,\n        "client_id": client.get_secret("fhir-client-id").value,\n        "private_key_jwk": client.get_secret("fhir-private-key").value,\n        "use_smart": True,\n        "scope": "system/*.read"\n    }\n'})}),"\n",(0,r.jsx)(n.h3,{id:"large-dataset-configuration",children:"Large dataset configuration"}),"\n",(0,r.jsx)(n.p,{children:"Large datasets may require parameter adjustments to the bulk export process:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Timeout and concurrency parameters can be increased for large datasets.\ndata_source = pc.read.bulk(\n    fhir_endpoint_url=fhir_endpoint,\n    types=resource_types,\n    since=since,\n    auth_config=auth_config,\n    timeout=7200,  # 2 hour timeout for large datasets.\n    max_concurrent_downloads=10\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"This guide describes the implementation of a synchronisation pipeline between\nFHIR bulk export endpoints and Delta tables using Pathling's built-in\nfunctionality. The approach provides several key characteristics:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Incremental processing"}),": The ",(0,r.jsx)(n.code,{children:"since"})," parameter enables efficient\nsynchronisation by retrieving only changed data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data merging"}),": Pathling's ",(0,r.jsx)(n.code,{children:"SaveMode.MERGE"})," automatically handles conflicts\nand updates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Error handling"}),": Built-in retry mechanisms address transient network and\nserver issues"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Code efficiency"}),": The core synchronisation logic requires minimal custom\nimplementation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Operational features"}),": Scheduling, monitoring, and security components\nsupport production deployment"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The combination of Pathling's bulk export capabilities and Delta Lake's merge\nfunctionality enables maintenance of current analytical datasets with limited\ncustom development effort while providing consistent synchronisation with FHIR\ndata sources."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);